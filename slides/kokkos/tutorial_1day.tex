%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}
  \frametitle{Hands-On 1 : query\_device}

  {\large\textcolor{red}{\textbf{Purpose:} just cross-checking Kokkos/Hwloc is working OK}}

  \begin{itemize}
  \item We will first re-use material from Kokkos github repository.
  \item On your home, on \texttt{ouessant}: 
    \begin{enumerate}
    \item \texttt{mkdir kokkos-tutorial; cd kokkos-tutorial}
    \item \texttt{git clone https://github.com/kokkos/kokkos.git} \\
      \# \textbf{Don't try to build kokkos here (for now)}
    %\item \texttt{git clone https://github.com/kokkos/kokkos-tutorials.git}
    %\item \texttt{cd kokkos-tutorials/1-Day-Tutorial}\\
    %  \# 1 Day tutorial exercice are routed to \textbf{build kokkos for you}
    \end{enumerate}
  \end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}[fragile=singleslide]
  \frametitle{Hands-On 1 : query\_device}

  {\large\textcolor{red}{\textbf{Purpose:} just cross-checking Kokkos/Hwloc is working OK}}

  \begin{itemize}
  \item Kokkos sources will be built by the application Makefile
  \item \texttt{cd \$HOME/kokkos-tutorial/example/query\_device}
  \item open \texttt{query\_device.cpp}; no computations, it just prints hardware information
  \item 
    \begin{enumerate}
    \item \textbf{Default serial build (with hwloc):} \texttt{make KOKKOS\_USE\_TPLS="hwloc"}\\
      How many NUMA / Cores / Hyperthreads on power8 CPU ?\\
      What is the current SMT mode on a ouessant login node ? (use command \texttt{ppc64\_cpu \--\--smt} or \texttt{ppc64\_cpu --info})
    \item \textbf{OpenMP build (with hwloc):} \texttt{make KOKKOS\_USE\_TPLS="hwloc" KOKKOS\_DEVICES=OpenMP} (off course, exact same information obtained)
    \item \textbf{CUDA/OpenMP build (with hwloc):} \texttt{make KOKKOS\_USE\_TPLS="hwloc" KOKKOS\_DEVICES=Cuda,OpenMP}; rerun and you should get information about the CPU+GPU configuration
    \end{enumerate}
  \item \textbf{Take some time to have a look at Makefile.}\\
    Note that latter when using an installed kokkos library, we won't need to set architecture or device related variables on the command line .
  \end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}[fragile=singleslide]
  \frametitle{Hands-On 1 : query\_device}

  {\large\textcolor{red}{\textbf{Purpose:} just cross-checking Kokkos/Hwloc is working OK}}

  \begin{itemize}
  \item \textcolor{orange}{\textbf{What happens if hwloc is not activated ?}}
  \item Edit file \texttt{query\_device.cpp} and do the following modification:
    \begin{enumerate}
    \item Add \texttt{Kokkos::initialize(argc, argv);} after \texttt{MPI\_Init}
    \item Add \texttt{Kokkos::finalize();} before \texttt{MPI\_Finalize}
    \item change\\
      {\small
        \begin{minted}{c++}
          #if defined( KOKKOS_HAVE_CUDA )
            Kokkos::Cuda::print_configuration( msg );
          #else
            Kokkos::OpenMP::print_configuration( msg );
          #endif
        \end{minted}
      }
    \end{enumerate}
  \item {\small Rebuild 1 \textcolor{red}{without HWLOC:} \texttt{make KOKKOS\_DEVICES=OpenMP}}
    {\small
      \begin{minted}{bash}
        Kokkos::OpenMP KOKKOS_HAVE_OPENMP thread_pool_topology[ 1 x 80 x 1 ]
      \end{minted}
    }
  \item {\small Rebuild 2 \textcolor{darkgreen}{with HWLOC:} \texttt{make KOKKOS\_DEVICES=OpenMP KOKKOS\_USE\_TPLS="hwloc"}}
    {\small 
      \begin{minted}{bash}
        hwloc( NUMA[2] x CORE[10] x HT[4] )
        Kokkos::OpenMP KOKKOS_HAVE_OPENMP hwloc[2x10x4] hwloc_binding_enabled thread_pool_topology[ 2 x 10 x 4 ]      
      \end{minted}
    }
    \item As already said: processor affinity is crucial to performance
  \end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}[fragile=singleslide]
  \frametitle{Kokkos data Container}

  \begin{itemize}
  \item \textcolor{blue}{\texttt{Kokkos::View<...>}} replacement for \texttt{std::vector} with multidimensionnal feature and hardware adapted memory layout\\
    \begin{itemize}
    \item \texttt{Kokkos::View<double **> data("data",NX,NY);} : 2D array with sizes known at runtime
    \item \texttt{Kokkos::View<double *[3]> data("data",NX);} : 2D array with first size known at runtime (NX), and second known at compile time (3).
    \item How do I access data ? $data(i,j)$ !
    \item Need to talk about \textbf{memory layout:}\\
      \begin{itemize}
      \item \textbf{LayoutLeft}: $data(i,j,k)$ uses linearized index as $i + NX*j + NX*NY * k$ (column-major order)
      \item \textbf{LayoutRight}: $data(i,j,k)$ uses linearized index as $k + NZ*j + NZ*NY * i$ (raw-major order)
      \item You can if you like, still enforce memory layout yourself (just use 1D Views, and compute index yourself);\\
        We will see the 2 possibilities with the miniApp on the Fisher equation
      \end{itemize}
    \item Which memory space ? The default one !\\
      Want to enforce in which memory space lives the view ? Kokkos::View<..., Kokkos::CudaSpace>
    \end{itemize}

  \end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}[fragile=singleslide]
  \frametitle{Kokkos data Container (2)}

  \begin{itemize}
  \item \textcolor{blue}{\texttt{Kokkos::View<...>}} are reference-counted
  \item By default do a \textcolor{orange}{\textbf{shallow copy}}
    \begin{minted}{c++}
      Kokkos::View<int *>("a",10);
      Kokkos::View<int *>("b",10);
      a = b; // a now points to b (ref counter incremented by 1)
    \end{minted}
  \item \textcolor{orange}{\textbf{Deep copy}} must by explicit:
    \begin{minted}{c++}
      Kokkos::deep_copy(dest,src);
    \end{minted}
    \begin{itemize}
    \item \textbf{Usefull when copying data from one memory space to another}\\
      e.g. \textcolor{red}{from HostSpace to CudaSpace}
    \item When \texttt{dest} and \texttt{src} are in the same memory space, it does nothing ! (usefull for portability, see example in miniapps later)
    \end{itemize}
  \end{itemize}

\end{frame}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}[fragile=singleslide]
  \frametitle{Kokkos data Container (3)}

  \begin{itemize}
  \item \textcolor{orange}{A verbose Kokkos::View declaration} example:
    \begin{minted}{c++}
      Kokkos::View<double*,Kokkos::LayoutLeft,Kokkos::CudaSpace> a;
    \end{minted}
    \begin{itemize}
    \item \textcolor{orange}{\textbf{What ?}} a data type
    \item \textcolor{orange}{\textbf{How ?}} a memory layout
    \item \textcolor{orange}{\textbf{Where ?}} a memory space
    \item the last two template parameters are optionnal (have default values)
    \end{itemize}
  \item \textcolor{blue}{\texttt{Kokkos::DualView<...>}} : usefull when porting an application incrementally, adata container on two different memory space.\\
    see \texttt{tutorial/Advanced\_Views/04\_dualviews/dual\_view.cpp}
  \item \textcolor{blue}{\texttt{Kokkos::UnorderedMap<...>}}
  \item Can also define \textbf{subview (array slicing, no deep copy)}. See exercice about Mandelbrot set.
  \end{itemize}
  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}[fragile=singleslide]
  \frametitle{Kokkos data Container (4)}

  \begin{itemize}
  \item \textcolor{red}{\textbf{What types of data may a View contain ?}}\\
    C++ \myhref{http://en.cppreference.com/w/cpp/concept/PODType}{Plain Old Data} (POD), i.e. basically compatible with C language:
    \begin{itemize}
    \item Can be allocated with \texttt{std::malloc}
    \item Can be copied with \texttt{std::memmove}
    \end{itemize}
  \item POD in C++11: 
    \begin{itemize}
    \item a trivial type (no virtual member functions, no virtual base class)
    \item a standard layout type
    \end{itemize}
  \item C++11: How to check if a given class \texttt{A} is POD ?
  \end{itemize}
  \begin{minted}{c++}
    #include <type_traits>
    
    class A { ... }
    std::cout << "is class A POD ? " << std::is_pod<A>::value << "\n";
  \end{minted}
  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}[fragile=singleslide]
  \frametitle{Kokkos compute Kernels}

  \begin{itemize}
  \item How to specify a compute kernel in Kokkos ?
    \begin{enumerate}
    \item \textcolor{blue}{\textbf{Use Lambda functions.}}\\
      NB: a lambda in c++11 is an unnamed function object capable of capturing variables in scope.
      \begin{minted}{c++}
        Kokkos::parallel_for (100, KOKKOS_LAMBDA (const int i) {
          data(i) = 2*i;
        });
      \end{minted}
      Here we do 2 things in 1 step: define the computation body (lambda func) and launch computation.
    \item \textcolor{darkgreen}{\textbf{Use a C++ functor class.}}\\
      A functor is a class containing a function to execute in parallel.
      \begin{minted}{c++}
        class FunctorType {
          public:
          KOKKOS_INLINE_FUNCTION
          void operator() ( const int i ) const ;
        };
        ...
        FunctorType func;
        Kokkos::parallel_for (100, func);
      \end{minted}
    \end{enumerate}
  \end{itemize}

\end{frame}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}[fragile=singleslide]
  \frametitle{Kokkos compute Kernels}

  \textbf{Lambda or Functor: which one to use in Kokkos ? Both !}
  \begin{enumerate}
  \item \textcolor{blue}{\textbf{Use Lambda functions.}}\\
    \begin{itemize}
    \item easy way for small compute kernels
    \item For GPU, requires Cuda 7.5 (8.0 is current and latest CUDA version)
    \end{itemize}
  \item \textcolor{darkgreen}{\textbf{Use a C++ functor class.}}\\
    \begin{itemize}
    \item More flexible, allow to design more complex kernel
    \end{itemize}
  \end{enumerate}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}[fragile=singleslide]
  \frametitle{Hands-On 2 : SAXPY}

  {\large \textcolor{red}{\textbf{Purpose:}} The simplest computing kernel in Kokkos, importance of hwloc}

  \begin{itemize}
  \item There 5 differents versions
  \item \textbf{1. Serial : no Kokkos)}
  \item \textbf{2. OpenMP : no Kokkos)}
  \item 3. Kokkos-Lambda-CPU : Kokkos with lambda for threads dispatch
  \item \textbf{4. Kokkos-Lambda : Kokkos with lambda for threads dispatch and data buffer (Kokkos::View)}
  \item 5. Kokkos-Functor-CPU : Kokkos with functor for threads dispatch only
  \end{itemize}
  
  \begin{itemize}
  %\item \textbf{Proposed activity}
  \item \textcolor{red}{\textbf{Saxpy serial (reference executable on Power8)}}
    \begin{itemize}
    \item \texttt{cd \$HOME/kokkos-tutorial/kokkos-tutorials/1-Day-Tutorial/Exercises/01\_AXPY/Serial}
    \item \texttt{make KOKKOS\_ARCH=Power8}
    \item Alternatively, we could have modify \texttt{Makefile} and change \texttt{SNB} into \texttt{Power8}
    \end{itemize}
  \item \textcolor{orange}{\textbf{Saxpy regular OpenMP (on Power8)}}
    \begin{itemize}
    \item \texttt{cd \$HOME/kokkos-tutorial/kokkos-tutorials/1-Day-Tutorial/Exercises/01\_AXPY/OpenMP}
    \item Rebuild: \texttt{make KOKKOS\_ARCH=Power8}; and observe performance
    \end{itemize}
  \end{itemize}

  \textbf{see also slides from SC2016, page 42(74).}
  
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}[fragile=singleslide]
  \frametitle{Hands-On 2 : SAXPY}

  \begin{itemize}
  \item \textcolor{violet}{\textbf{Saxpy Kokkos OpenMP (on Power8)}}    
    \begin{itemize}
    \item \texttt{cd \$HOME/kokkos-tutorial/kokkos-tutorials/1-Day-Tutorial/Exercises/01\_AXPY/Kokkos-Lambda}
    \item Add 3 lines in \texttt{saxpy.cpp} right after Kokkos initialization
      \begin{minted}{c++}
        std::ostringstream msg;
        Kokkos::OpenMP::print_configuration( msg );
        std::cout << msg.str();
      \end{minted}
    \item \texttt{make KOKKOS\_ARCH=Power8}
    \item Make sure all available CPU cores were used ($1\times 160 \times 1$)
    \item Change the number of OpenMP threads created by kokkos, e.g. :\\
      \texttt{./saxpy.host  --threads=20}
    \item Add again \texttt{KOKKOS\_USE\_TPLS="hwloc"} on the command line\\
      Rebuild and rerun, you should see that application uses \textbf{all the available numa domains}, and a strongly increased bandwidth usage !
    \end{itemize}
  \end{itemize}

\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% 
\begin{frame}[fragile=singleslide]
  \frametitle{Hands-On 2 : SAXPY}

  \begin{itemize}
  \item \textcolor{darkgreen}{\textbf{Saxpy CUDA (on Power8 + Nvidia K80/P100)}}
    \begin{itemize}
    \item \texttt{cd \$HOME/kokkos-tutorial/kokkos-tutorials/1-Day-Tutorial/Exercises/01\_AXPY/Kokkos-Lambda}
    \item \texttt{module load cuda/8.0}
    \end{itemize}
  \item Rebuild for K80, run on ouessant (front node):\\
    \texttt{make KOKKOS\_DEVICES="Cuda,OpenMP" KOKKOS\_ARCH="Kepler37,Power8" KOKKOS\_USE\_TPLS="hwloc"}
  \item Rebuild for P100, run on compute node using \texttt{submit\_ouessant.sh} (should see a strong difference):\\
    \texttt{make KOKKOS\_DEVICES="Cuda,OpenMP" KOKKOS\_ARCH="Pascal60,Power8" KOKKOS\_USE\_TPLS="hwloc"}\\
    Please note that \textbf{maximun bandwith is 732 GB/s for Pascal P100}, you can retrieve this number by examining \texttt{deviceQuery} example in CUDA/SDK.
  \end{itemize}
\end{frame}

